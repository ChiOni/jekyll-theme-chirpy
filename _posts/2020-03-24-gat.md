---
title: Graph Attention Networks (Pytorch)
date: 2020-03-24 00:00:00 +0800
categories: [Code Excercise, Graph Neural Networks]
tags: [graph neural network, node classification, attention]
seo:
  date_modified: 2020-03-24 20:07:02 +0800
---

컨셉을 이해하여 고개를 끄덕인 후, 그래서 어떻게 구현하지? 라고 생각해봤을 때 갑자기 멍해지는 기분에 불쾌한 경우가 자주 있다. 이론의 직관성과 멍함의 정도가 비례한다면 어텐션은 아주아주 기분 나쁜 개념이다. 어텐션은 대체 어떻게 구현하지?  

<img src="/assets/img/pe/gat/gatone.jpg">  
# <b>Attention Network</b>
우선 어텐션은 뭘까? 내가 이해한 어텐션의 컨셉은 집중이다. 많은 정보가 산재해있을 때, 무엇을 집중해서 볼지에 대한 것을 휴리스틱하지 않게 네트워크에 맡기자는 컨셉이라 볼 수 있다.   

그렇다면 딥러닝에서 어텐션은 어떻게 쓰일까? 위의 사진은 시퀀스의 형태로 데이터의 정보가 전달되는 RNN 네트워크이다. 그런데 단순히 일방향으로 정보를 전달하는 것이 아니라 C(t) 라는 어텐션 벡터를 마지막 의사결정 직전에 첨가한다. 이미 네트워크를 흐르며 종합한 s ~ t 까지의 정보를 마지막 순간에 다시 한 번 취합한다. 논문을 읽지는 않았지만 네트워크를 흐르며 손실된 Long-Term Memory를 보존하고자 하는 의도였을 것 같다. 

> 그럼 그냥 이전 네트워크에서 넘겨준 매트릭스를 남겨줬다가 나중에 어떤 계수를 곱해서 다시 더해주는 것일까? 그러면 Attention은 상수인가? 그럼 Set의 모든 데이터 각각의 특성을 어떻게 고려하지? 어떤 데이터는  t-1을 많이 집중해야되고, 어떤 데이터는 t-2를 집중해야하는데, 그런걸 어떻게 구현하지?  

사실 어텐션을 어떻게 적용할지는 공식적으로 정해져 있는 것이 아니다. 모델의 구조나 목적하는 기능에 따라 여러가지 어텐션의 형태가 존재할 수 있는데, 오늘은 그 중 비교적 쉽고 직관적으로 구현해볼 수 있는 Graph Attention Netowrks를 구현해보자.  



# Graph Attention Networks (ICLR 2017)  
2017년에 등장한 어텐션 논문을 한 편 구현해보자. 사실 Attention Is All You Need라는 어텐션 괴물 논문을 구현해보고 싶지만 어려워보이니깐 나중으로 미루기! 논문 리뷰가 아닌 코드 구현이 목적이기 때문에 실험이나 논리적 컨셉에 대한 것은 제쳐두자. 전체적인 알고리즘과 구체적인 어텐션에 집중하자.  
<img src="/assets/img/pe/gat/gattwo.jpg">  
위의 이미지에 GAT의 모든 컨셉이 담겨져있다.  
- h1은 주위의 다른 노드들에게 영향을 받아 h1'으로 진화한다.  
- 어텐션을 h2,h3 ... h6에게만 받는 것이 아니라 h1 자기 자신에게도 받는다.  
- 어텐션이 파랑색, 보라색, 초록색 여러 가지 색깔로 표현되어 있다.
  - convolution을 multi-channel로 하는 것과 같이 어텐션도 <b>multi-head attention을</b> 사용한다.  

<br/>

<img src="/assets/img/pe/gat/gatthree.jpg">
위의 이미지로 GAT 네트워크를 구체적으로 이해할 수 있다.  

<b>Expression 1.</b>  

- e(ij) 는 i 노드와 j 노드의 Attention Coefficients
- 각 h(i), h(j) 앞에 붙어 있는 W는 F x F' 크기의 매트릭스로 각 노드를 임베딩하는 매트릭스
- 각 노드는 F개의 피쳐가 있는데 임베딩을 통해 F' Dimension으로 바뀌게 된다.

<b>Expression 2.</b>  

- 분모의 N(i)는 i 노드에 연결된 이웃 노드들의 집합 (i 도 포함된다)
- 알파가 의미하는 것은 이웃들의 전체 coefficient 합 중 e(ij)가 차지하는 비율(혹은 확률)

<b>Expression 3.</b>  

- 조금 더 구체적으로 Attention Coefficient가 어떻게 나타나는지 표현되어 있다.
- 식의 안쪽에서부터 살펴보면 [Wh(i) // Wh(j)]가 의미하는 것은 두 임베딩 벡터의 Concat이다.
- F' + F' 하여 2F' 길이의 벡터를 single-layer로 forward 시킨다.
- LeakyRelu는 음수에 대해서도 어느정도 값을 취하는 nonlinear activation  function

<b>Expression 4.</b>  

- F 개 피쳐의 h는 주변 노드들의 (attention * F') 들의 합인 h'로 재정의 된다.

<b>Expression 5.</b>  

- GAT 에서는 concat 한 임베딩 벡터를 feed forward 하는 어텐션 네트워크를 K개 가진다.
- <b>Attention Is All You Need (2017)</b> 에서 제안된 multi-head attention mechanism
- F' 길이의 벡터를 K번 Concat 하여 K * F' 길이의 벡터를 얻는다.

<b>Expression 6.</b>  

- 만약 h' 뒤에 output을 위한 fc layer가 추가되는 것이 아닐 때 취하는 구조
- Expression 5와 같이 concat 하는 것이 아니라 K개의 F' 길이 벡터들을 합해준 뒤 평균

<br/>

여기까지 왔다면 GAT의 모든 것을 이해했다고 볼 수 있다. 사실 GAT의 가장 큰 장점은 모델의 간결함이라 생각이 든다. 어텐션, 임베딩, 그리고 예측 까지의 모든 과정이 사실은 모두 익숙한 FC - Layer의 일종일 뿐이다. 당시 이렇게 간단한 모델로 많은 Task에서 SOTA의 성능을 보였으니 모두가 어텐션에 열광할 수 밖에.  

# Deep Graph Library

본격적으로 GAT를 구현하기에 앞서, dgl 이라는 라이브러리를 하나 보고 가자. 이름에서 유추할 수 있듯이 Graph Neural Network를 구현하는데 도움을 주는 라이브러리인데, tensor 베이스의 Pytorch나 MXNet에서 사용 가능하다. 데이터 로드부터 네트워크 모듈까지 아주 다양한 기능을 제공하는 유용한 라이브러리인데 keras를 사용하는 것과 비슷한 이유에서 유용하다.  

```python
class GCN_layer(nn.Module):
    def __init__(self, in_features, out_features, A):
        super(GCN_layer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.A = A
        self.fc = nn.Linear(in_features, out_features)
        
    def forward(self, X):
        return self.fc(torch.spmm(self.A, X))
    
 
# 이전에 GCN 네트워크를 Pytorch로 구현할 때
# 그래프 구조의 데이터를 취합하기 위하여 adjency matrix A를 생성
# DGL이 있다면 이것을 아래와 같이 대체할 수 있다고 한다.

import dgl.function as fn

gcn_msg = fn.copy_src(src='h', out='m')
gcn_reduce = fn.sum(msg='m', out='h')
update_all(gcn_msg, gcn_reduce)
```

솔직히 말하면 아직 익숙하지가 않아서 하나 하나의 함수들이 뭘 뜻하는지는 잘 모르겠다. 우선 다양한 그래프들을 손쉽게 텐서 형태의 데이터로 로드할 수 있다는 장점만으로도 dgl이나 torch_gemetric과 같은 라이브러리와 친해질 필요가 있을 것 같다. 그럼 드디어 dgl을 사용하여 데이터를 로드하고 GAT를 구현해보자.  

# PyTorch implentation of the GAT









