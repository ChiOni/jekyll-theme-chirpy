---
title: Graph Attention Networks (Pytorch)
date: 2020-03-24 00:00:00 +0800
categories: [Code Excercise, Graph Neural Networks]
tags: [graph neural network, node classification, attention]
seo:
  date_modified: 2020-03-24 20:07:02 +0800
---

컨셉을 이해하여 고개를 끄덕인 후, 그래서 어떻게 구현하지? 라고 생각해봤을 때 갑자기 멍해지는 기분에 불쾌한 경우가 자주 있다. 이론의 직관성과 멍함의 정도가 비례한다면 어텐션은 아주아주 기분 나쁜 개념이다. 어텐션은 대체 어떻게 구현하지?  

<img src="/assets/img/pe/gat/gatone.jpg">  
# <b>Attention Network</b>
우선 어텐션은 뭘까? 내가 이해한 어텐션의 컨셉은 집중이다. 많은 정보가 산재해있을 때, 무엇을 집중해서 볼지에 대한 것을 휴리스틱하지 않게 네트워크에 맡기자는 컨셉이라 볼 수 있다.   

그렇다면 딥러닝에서 어텐션은 어떻게 쓰일까? 위의 사진은 시퀀스의 형태로 데이터의 정보가 전달되는 RNN 네트워크이다. 그런데 단순히 일방향으로 정보를 전달하는 것이 아니라 C(t) 라는 어텐션 벡터를 마지막 의사결정 직전에 첨가한다. 이미 네트워크를 흐르며 종합한 s ~ t 까지의 정보를 마지막 순간에 다시 한 번 취합한다. 논문을 읽지는 않았지만 네트워크를 흐르며 손실된 Long-Term Memory를 보존하고자 하는 의도였을 것 같다. 

> 그럼 그냥 이전 네트워크에서 넘겨준 매트릭스를 남겨줬다가 나중에 어떤 계수를 곱해서 다시 더해주는 것일까? 그러면 Attention은 상수인가? 그럼 Set의 모든 데이터 각각의 특성을 어떻게 고려하지? 어떤 데이터는  t-1을 많이 집중해야되고, 어떤 데이터는 t-2를 집중해야하는데, 그런걸 어떻게 구현하지?  
<br/>
사실 어텐션을 어떻게 적용할지는 공식적으로 정해져 있는 것이 아니다. 모델의 구조나 목적하는 기능에 따라 여러가지 어텐션의 형태가 존재할 수 있는데, 오늘은 그 중 비교적 쉽고 직관적으로 구현해볼 수 있는 Graph Attention Netowrks를 구현해보자.

<br/>

# Graph Attention Networks (ICLR 2017)  
2017년에 등장한 어텐션 논문을 한 편 구현해보자. 사실 Attention Is All You Need라는 어텐션 괴물 논문을 구현해보고 싶지만 어려워보이니깐 나중으로 미루기! 논문 리뷰가 아닌 코드 구현이 목적이기 때문에 실험이나 논리적 컨셉에 대한 것은 제쳐두자. 전체적인 알고리즘과 구체적인 어텐션에 집중하자.  
